package com.ibcai.writer;

import com.ibcai.common.Cfg;
import io.lettuce.core.api.sync.RedisCommands;
import org.springframework.data.mongodb.core.BulkOperations;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.Duration;
import java.time.Instant;
import java.util.*;
import java.util.concurrent.atomic.AtomicLong;

/**
 * æ‰¹é‡WriteræœåŠ¡ - æ­¥éª¤6ï¼šä»Redisæ‰¹è¯»å¹¶æ‰¹å†™MongoDB
 * 
 * è¦æ±‚ï¼š
 * - è§¦å‘æ¡ä»¶ï¼šWRITER_BATCH_SIZE=100ï¼ˆæ•°é‡ï¼‰æˆ– WRITER_BATCH_INTERVAL_MS=120000ï¼ˆ2åˆ†é’Ÿï¼‰
 * - ä»Redisæ‰¹è¯» â†’ æ‰¹å†™MongoDB
 * - ä¸ä¿®æ”¹ç°æœ‰æ•°æ®æ¨¡å‹/å¯¹å¤–æ¥å£
 * - å¹‚ç­‰/é‡å¤é¿å…ç­–ç•¥ä¸ç°æœ‰é€»è¾‘ä¸€è‡´
 */
public class BatchWriter {
    
    private static final Logger log = LoggerFactory.getLogger(BatchWriter.class);
    
    private final RedisCommands<String, String> redis;
    private final MongoTemplate mongo;
    private final Map<String, Object> config;
    
    // é…ç½®å‚æ•°
    private final boolean enabled;
    private final int batchSize;
    private final long batchIntervalMs;
    private final int maxPerFlush;
    private final List<String> supportedTopics;
    private final Map<String, String> topicCollectionMap;
    
    // è¿è¡ŒçŠ¶æ€
    private volatile boolean running = false;
    private Thread writerThread;
    private final Map<String, Long> lastFlushMap = new HashMap<>();
    
    // ç»Ÿè®¡è®¡æ•°å™¨
    private final AtomicLong totalProcessed = new AtomicLong(0);
    private final AtomicLong totalBatches = new AtomicLong(0);
    private final AtomicLong totalErrors = new AtomicLong(0);
    
    public BatchWriter(RedisCommands<String, String> redis, MongoTemplate mongo, Map<String, Object> config) {
        this.redis = redis;
        this.mongo = mongo;
        this.config = config;
        
        // è¯»å–é…ç½®
        Map<String, Object> writerConfig = (Map<String, Object>) config.getOrDefault("writer", new HashMap<>());
        
        this.enabled = Cfg.get(writerConfig, "enabled", true);
        this.batchSize = Cfg.get(writerConfig, "batchSize", 100);
        
        // å®‰å…¨å¤„ç† Long ç±»å‹è½¬æ¢
        Number batchIntervalNum = Cfg.get(writerConfig, "batchIntervalMs", 120000);
        this.batchIntervalMs = batchIntervalNum.longValue();
        
        this.maxPerFlush = Cfg.get(writerConfig, "maxPerFlush", 500);
        
        this.supportedTopics = (List<String>) writerConfig.getOrDefault("topics", 
                Arrays.asList("state", "connection", "networkIp", "error", "cargo"));
        
        Map<String, String> collectionsConfig = (Map<String, String>) writerConfig.getOrDefault("collections", new HashMap<>());
        this.topicCollectionMap = new HashMap<>(collectionsConfig);
        
        // åˆå§‹åŒ–æœ€ååˆ·æ–°æ—¶é—´
        long currentTime = System.currentTimeMillis();
        for (String topic : supportedTopics) {
            lastFlushMap.put(topic, currentTime);
        }
        
        log.info("ğŸ”§ BatchWriter initialized: enabled={}, batchSize={}, batchIntervalMs={}, topics={}", 
                enabled, batchSize, batchIntervalMs, supportedTopics);
    }
    
    /**
     * å¯åŠ¨æ‰¹é‡Writer
     */
    public void start() {
        if (!enabled) {
            log.info("ğŸ“‹ BatchWriter: disabled, not starting");
            return;
        }
        
        if (running) {
            return;
        }
        
        running = true;
        writerThread = new Thread(this::writerLoop, "batch-writer");
        writerThread.setDaemon(true);
        writerThread.start();
        
        log.info("ğŸš€ BatchWriter started");
    }
    
    /**
     * åœæ­¢æ‰¹é‡Writer
     */
    public void stop() {
        running = false;
        if (writerThread != null) {
            writerThread.interrupt();
        }
        log.info("ğŸ›‘ BatchWriter stopped");
    }
    
    /**
     * Writerä¸»å¾ªç¯
     */
    private void writerLoop() {
        while (running && !Thread.currentThread().isInterrupted()) {
            try {
                processAllTopics();
                Thread.sleep(1000); // 1ç§’æ£€æŸ¥é—´éš”
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            } catch (Exception e) {
                log.error("âŒ Error in BatchWriter loop: {}", e.getMessage(), e);
                totalErrors.incrementAndGet();
                
                try {
                    Thread.sleep(5000); // é”™è¯¯åç­‰å¾…5ç§’
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    break;
                }
            }
        }
    }
    
    /**
     * å¤„ç†æ‰€æœ‰æ”¯æŒçš„ä¸»é¢˜
     */
    private void processAllTopics() {
        for (String topic : supportedTopics) {
            try {
                processTopicBatches(topic);
            } catch (Exception e) {
                log.error("âŒ Error processing topic {}: {}", topic, e.getMessage());
                totalErrors.incrementAndGet();
            }
        }
    }
    
    /**
     * å¤„ç†ç‰¹å®šä¸»é¢˜çš„æ‰¹é‡æ•°æ®
     */
    private void processTopicBatches(String topic) {
        // æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„Redisé”®
        String pattern = "ingest:" + topic + ":*";
        List<String> keyList = redis.keys(pattern);
        Set<String> keys = new HashSet<>(keyList);
        
        if (keys.isEmpty()) {
            return;
        }
        
        long currentTime = System.currentTimeMillis();
        long lastFlush = lastFlushMap.get(topic);
        boolean timeTriggered = (currentTime - lastFlush) >= batchIntervalMs;
        
        // è®¡ç®—æ€»çš„å¾…å¤„ç†æ¶ˆæ¯æ•°
        long totalMessages = 0;
        for (String key : keys) {
            totalMessages += redis.llen(key);
        }
        
        boolean sizeTriggered = totalMessages >= batchSize;
        
        if (sizeTriggered || timeTriggered) {
            processBatch(topic, keys, timeTriggered ? "TIME" : "SIZE");
            lastFlushMap.put(topic, currentTime);
        }
    }
    
    /**
     * å¤„ç†æ‰¹é‡æ•°æ®
     */
    private void processBatch(String topic, Set<String> keys, String triggerReason) {
        List<String> batch = new ArrayList<>();
        
        // ä»æ‰€æœ‰ç›¸å…³é”®ä¸­æ”¶é›†æ¶ˆæ¯
        for (String key : keys) {
            long keyLen = redis.llen(key);
            int toTake = (int) Math.min(keyLen, maxPerFlush - batch.size());
            
            for (int i = 0; i < toTake; i++) {
                String message = redis.rpop(key);
                if (message != null) {
                    batch.add(message);
                }
            }
            
            if (batch.size() >= maxPerFlush) {
                break;
            }
        }
        
        if (!batch.isEmpty()) {
            writeToMongoDB(topic, batch, triggerReason);
        }
    }
    
    /**
     * å†™å…¥MongoDB
     */
    private void writeToMongoDB(String topic, List<String> batch, String triggerReason) {
        String collectionName = topicCollectionMap.getOrDefault(topic, topic + "_events");
        
        try {
            Instant startTime = Instant.now();
            
            BulkOperations ops = mongo.bulkOps(BulkOperations.BulkMode.UNORDERED, collectionName);
            
            for (String json : batch) {
                Map<String, Object> doc = new HashMap<>();
                doc.put("raw", json);
                doc.put("ingestedAt", new Date());
                doc.put("topic", topic);  // æ·»åŠ ä¸»é¢˜ä¿¡æ¯
                doc.put("source", "step6-batch-writer");  // æ ‡è¯†æ•°æ®æ¥æº
                ops.insert(doc);
            }
            
            ops.execute();
            
            long durationMs = Duration.between(startTime, Instant.now()).toMillis();
            
            totalProcessed.addAndGet(batch.size());
            totalBatches.incrementAndGet();
            
            log.info("âœ… BatchWriter[{}]: Flushed {} messages to {} in {}ms (trigger={})", 
                    topic, batch.size(), collectionName, durationMs, triggerReason);
            
        } catch (Exception e) {
            log.error("âŒ Failed to write batch to MongoDB for topic {}: {}", topic, e.getMessage(), e);
            totalErrors.incrementAndGet();
            
            // å¤±è´¥æ—¶å°†æ¶ˆæ¯é‡æ–°æ”¾å›Redisï¼ˆç®€å•é‡è¯•ç­–ç•¥ï¼‰
            for (String message : batch) {
                String key = "ingest:" + topic + ":retry";
                redis.lpush(key, message);
            }
        }
    }
    
    /**
     * è·å–ç»Ÿè®¡ä¿¡æ¯
     */
    public String getStats() {
        return String.format("BatchWriter[processed=%d, batches=%d, errors=%d, running=%s]", 
                           totalProcessed.get(), totalBatches.get(), totalErrors.get(), running);
    }
    
    /**
     * æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ
     */
    public boolean isRunning() {
        return running;
    }
}
